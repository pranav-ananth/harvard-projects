{"cells":[{"cell_type":"code","source":["#@title Helper Code: run this cell\n","\n","# These helper code functions call OpenAI APIs in order to use pre-trained OpenAI Large Language Models.\n","\n","import os\n","import sys\n","# For hiding outputs, warnings, etc.\n","class HiddenPrints:\n","    def __enter__(self):\n","        self._original_stdout = sys.stdout\n","        sys.stdout = open(os.devnull, 'w')\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        sys.stdout.close()\n","        sys.stdout = self._original_stdout\n","\n","# These helper code functions call OpenAI APIs in order to use pre-trained OpenAI Large Language Models.\n","print(\"Loading models...\")\n","\n","with HiddenPrints():\n","    !pip install openai\n","    !pip install datasets\n","    !pip install -qU langchain\n","    !pip install -qU openai\n","    !pip install -qU \\\n","        datasets==2.12.0 \\\n","        apache_beam \\\n","        mwparserfromhell\n","    # pip install pip==21.3.1\n","\n","    !pip install -qU \\\n","    langchain==0.0.162 \\\n","    openai==0.27.7 \\\n","    tiktoken==0.4.0 \\\n","    \"pinecone-client[grpc]\"==2.2.1\n","\n","    !pip install langchain openai\n","\n","    from gensim.models import Word2Vec\n","    from gensim.models.word2vec import LineSentence\n","    from gensim.test.utils import common_texts\n","    import nltk\n","    import requests\n","    import openai as ai\n","    # from datasets import load_dataset\n","    import pandas as pd\n","    import json\n","\n","    # Students will need to get their own API key.\n","    # api_key = \"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\"\n","    # ai.api_key = \"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\"\n","    # API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\n","    pinecone_api = \"b597e2dd-4bc6-4f90-befc-faa372b1be11\"\n","    pinecone_env = \"us-west1-gcp-free\"\n","\n","import os\n","\n","# os.environ['OPENAI_API_KEY'] = \"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\"\n","\n","# Word2Vec\n","model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# This function will generate a GPT Response for older models, for example \"text-davinci-002\" or \"text-davinci-003\"\n","def generate_previous_gpt_model_response(MODEL, PROMPT, MAX_TOKENS=250, TEMP=0.99, TOP_P=1, N=1, FREQ_PEN=0.3, PRES_PEN = 0.9):\n","  response = ai.Completion.create(\n","          engine = MODEL,\n","          # engine=\"text-davinci-002\", # OpenAI has made four text completion engines available, named davinci, ada, babbage and curie. We are using davinci, which is the most capable of the four.\n","          prompt=PROMPT, # The text file we use as input (step 3)\n","          max_tokens=MAX_TOKENS, # how many maximum characters the text will consists of.\n","          temperature=TEMP,\n","          # temperature=int(temperature), # a number between 0 and 1 that determines how many creative risks the engine takes when generating text.,\n","          top_p=TOP_P, # an alternative way to control the originality and creativity of the generated text.\n","          n=N, # number of predictions to generate\n","          frequency_penalty=FREQ_PEN, # a number between 0 and 1. The higher this value the model will make a bigger effort in not repeating itself.\n","          presence_penalty=PRES_PEN # a number between 0 and 1. The higher this value the model will make a bigger effort in talking about new topics.\n","      )\n","  return response['choices'][0]['text']\n","\n","# For GPT-3.5\n","def generate_newer_gpt_model_response(model, prompt, TEMP=1, max_tokens=None):\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"Authorization\": f\"Bearer {api_key}\",\n","    }\n","\n","    data = {\n","        \"model\": model,\n","        \"messages\": [{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n","        \"temperature\": TEMP,\n","    }\n","\n","    if max_tokens is not None:\n","        data[\"max_tokens\"] = max_tokens\n","\n","    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))\n","\n","    if response.status_code == 200:\n","        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n","    else:\n","        raise Exception(f\"Error {response.status_code}: {response.text}\")\n","\n","# Load Bias Dataset\n","# dataset = load_dataset(\"md_gender_bias\", \"convai2_inferred\")"],"metadata":{"id":"CkaUqG8wGS0I","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Enter your API Key here!\n","API_KEY = \"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\" # @param {type:\"string\"}\n","api_key = API_KEY\n","ai.api_key = API_KEY\n","\n","API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\n","\n","os.environ['OPENAI_API_KEY'] = API_KEY"],"metadata":{"cellView":"form","id":"1pmYkIL2UvLd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. How Reinforcement Learning with Human Feedback (RLHF) works"],"metadata":{"id":"une38aguOZ-R"}},{"cell_type":"markdown","source":["- Reinforcement Learning from Human Feedback is a way of aligning LLMs with human values.\n","\n","- It often involves human labellers labelling a training set of LLM responses with a rating of whether that is a good response.\n","\n","- This labelled dataset is then used to train the model further by giving the model a reward if it predicts the next words in a sentence in a way which lines up with a high labeller score.\n","\n","- The model then learns to predict next words that are more likely to align with what the human labellers said.\n","\n","- This is called fine-tuning a model with Reinforcement Learning from Human feedback.\n","\n","- Based on the above explanation draw a diagram of RLHF [here](https://excalidraw.com/#json=WNEjp-vxJ_le3wBU-7QuG,R2mZskL9r85okIQGzZcUtg):"],"metadata":{"id":"AFnAIV6hgV_G"}},{"cell_type":"markdown","source":["## 2. Understanding GPT Model Parameters"],"metadata":{"id":"1XOPJYC9E2MI"}},{"cell_type":"markdown","source":["There are a number of parameters you can use to prompt GPT models.\n","\n","Some important parameters include:\n","\n","**temperature:** Temperature is a number between 0 and 1 that determines how many creative risks the engine takes when generating text. 0 is least creative and 1 is most creative.\n","\n","**max_tokens:** How many maximum characters the text will consists of.\n","\n","**top_p:** an alternative way to control the originality and creativity of the generated text.\n","\n","**frequency_penalty:** a number between 0 and 1. The higher this value the model will make a bigger effort in not repeating itself."],"metadata":{"id":"RTOsqTG5E5_I"}},{"cell_type":"code","source":["# Can you spot any differences between a temperature of 0 and 1?\n","model=\"gpt-3.5-turbo\"\n","prompt = \"I once said my favourite animal is\"\n","\n","for i in range(5):\n","  response_1 = generate_newer_gpt_model_response(model, prompt, TEMP=0)\n","\n","  response_2 = generate_newer_gpt_model_response(model, prompt, TEMP=1)\n","\n","  print(\"*******\")\n","  print(f\"Temperature 0 response: {response_1}\")\n","  print(f\"Temperature 1 response: {response_2}\")"],"metadata":{"id":"Hczo5Cv9nD_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Experiment with some prompts which exaggerate the differences in temperature. You could add these to your presentation."],"metadata":{"id":"MTGHhbQB55K7"}},{"cell_type":"markdown","source":["## 3. Hallucinations"],"metadata":{"id":"8g5Nb_tRQCuF"}},{"cell_type":"markdown","source":["Read the outputs to prompts below."],"metadata":{"id":"Ti7ql027CfH_"}},{"cell_type":"code","source":["prompt = \"\"\"\n","Provide a bibliography for studying the relationship between Machine Learning and sport\n","\"\"\"\n","model=\"davinci\"\n","generate_previous_gpt_model_response(model, prompt)"],"metadata":{"id":"3IJPfAIV98n-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"\n","Provide a bibliography for studying the relationship between Machine Learning and sport\n","\"\"\"\n","model=\"gpt-3.5-turbo\"\n","output = generate_newer_gpt_model_response(model, prompt)\n","print(output)"],"metadata":{"id":"ADqFJLUUx6ky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Which is of these models provides a better response? Why?"],"metadata":{"id":"Oc4W0QP4_ECL"}},{"cell_type":"markdown","source":["Why do models hallucinate in this way?"],"metadata":{"id":"XNqxwOOEAT9s"}},{"cell_type":"markdown","source":["Experiment with OpenAI models to see if you can find examples of hullucination. You could add these to your presentation."],"metadata":{"id":"9RHfsM0u5_uu"}},{"cell_type":"markdown","metadata":{"id":"KCKx1-txi9K0"},"source":["## 4. LLMs and Democracy"]},{"cell_type":"markdown","metadata":{"id":"ZmoBIk1bq89s"},"source":["Read the following Article: https://www.dair-institute.org/blog/letter-statement-March2023"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLpsKFcJ3OiB","colab":{"base_uri":"https://localhost:8080/","height":505},"cellView":"form","executionInfo":{"status":"ok","timestamp":1689172783033,"user_tz":420,"elapsed":14,"user":{"displayName":"Sarthak Kanodia","userId":"05250390221850813484"}},"outputId":"3e2418ef-0cee-4711-b276-ff2d8d82080a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://drive.google.com/file/d/1MYVPcKcAAkik7O-mt9w47TRZyoNPgDZG/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>\n"]},"metadata":{}}],"source":["#@title AI Researcher Timnit Gebru\n","%%html\n","<iframe src=\"https://drive.google.com/file/d/1MYVPcKcAAkik7O-mt9w47TRZyoNPgDZG/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHxjgvPfrFxq","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["efeeed2e6245494ab7844791f5b94c04","018e0afc5f5e4bf0800fcab896b07375","631efaa337d8496184586b3b6291f64d"]},"cellView":"form","executionInfo":{"status":"ok","timestamp":1692678197758,"user_tz":420,"elapsed":177,"user":{"displayName":"Sarthak Kanodia","userId":"05250390221850813484"}},"outputId":"e4a9abb1-cc00-43d4-ccb4-e9b96c275efd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["IntSlider(value=5, max=10)"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efeeed2e6245494ab7844791f5b94c04"}},"metadata":{}}],"source":["#@title On A scale from 1-10 how far do you agree with Timnit Gebru and co-authors about the need for regulation?\n","\n","import ipywidgets as widgets\n","slider = widgets.IntSlider(value=5, max=10)\n","display(slider)"]},{"cell_type":"markdown","metadata":{"id":"5SaUG2Ca1iZz"},"source":["## 5. Model-in-the-loop to improve accuracy"]},{"cell_type":"markdown","metadata":{"id":"HkQpgn6DHR8o"},"source":["One alternative (or supplement) to RLHF that has been explored by the company Anthropic is called ['Constitutional AI'](https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations). This is where one AI is used to give feedback to another AI based on a set of rules, or [constitution](https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic_ConstitutionalAI_v2.pdf). The following activity is a small-scale experiment to use an AI to critique and give feedback to another AI.\n","\n","**Model-in-the-loop:** This means when a model is used to give feedback to another model."]},{"cell_type":"markdown","source":["####**ACTIVITY**\n","\n","First use GPT to generate a response to help a student learn about transformers.\n","\n","Secondly, use a seperate GPT reponse to assess how accurate the information is and output this.\n","\n","You could build this and a check for whether this techinque works!\n","\n","For more details on this approach click [here](https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations)"],"metadata":{"id":"XBiPREn-6TC6"}},{"cell_type":"code","source":["#Write your code here\n","model=\"gpt-3.5-turbo\"\n","prompt = \"ADD YOUR PROMPT HERE\"\n","response_1 = None #ADD CODE HERE\n","\n","prompt_checker = f\"Evaluate whether this response accurately explains the concept of transformer models and why: {response_1}\"\n","response_checker = None #ADD CODE HERE\n","\n","print(response_1)\n","print(f\"Here is an analysis of the accuracy of the response: {response_checker}\")"],"metadata":{"id":"2-IR0ZO9h70J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqqoXQbP6LWv"},"source":["**Discussion:** This is a basic form of a technique used by the AI company Anthropic. Do you think this would be effective? Why/ Why not?"]},{"cell_type":"markdown","source":["## 6. Making your app more reliable.\n","\n","Recap the ways you have learned to use LLMs more reliably:\n","\n","1. Prompting (Priming, Few Shot Promting, Chain of Thought or Tree of Thoughts)\n","\n","2. Chains\n","\n","3. Parameters (Especially Temperature)\n","\n","4. Choose a less biased model (Based on your research of the bias of the models)\n","\n","5. Use a model-in-the-loop\n","\n","Please note that these techniques reduces the unreliability of LLMs but these models are still inherently very unreliable."],"metadata":{"id":"hvGExfWiiqlD"}},{"cell_type":"markdown","metadata":{"id":"lV9Ga1oa5VtP"},"source":["## 7. Some risks to consider"]},{"cell_type":"markdown","source":["Checklist before building your app:\n","\n","- What is your intention for this app?\n","- How important is bias in your app? Where could bias come in?\n","- How important is accuracy in your app? Where could hullucinations come in?\n","- Does your app involve reasoning or exploration."],"metadata":{"id":"th16_jRrfzqw"}},{"cell_type":"markdown","metadata":{"id":"tunbbQl_5y9D"},"source":["[**Prompt Injection**](https://learnprompting.org/docs/prompt_hacking/injection): This is when a user uses the prompt to get round the app (for example try to find out the prompt you have used). You can therefore test unusual prompts with your app to see if this is a risk.\n","\n","**Factual Grounding:** This is where the model 'hullucinates' and makes up material. For example, this includes models referencing academic papers which don't exist and generating characteristics of peoples' lives which never happened.\n","\n","**Bias:** This is where the model has learned gender, racial or other biases from its training data."]},{"cell_type":"markdown","metadata":{"id":"FD9f7iTn54dI"},"source":["**Discussion:** How could you try to prevent these in creating an app with an LLM?"]},{"cell_type":"markdown","metadata":{"id":"TIj239tnm_NO"},"source":["## 8. Build an app with GPT"]},{"cell_type":"markdown","metadata":{"id":"nNYc26vYJLlS"},"source":["Today we will be using Streamlit, a framework to easily build web applications, to deploy our models to the web so that they can be shared to the web!\n","\n","Take a moment to look through examples of websites built with Streamlit [here](https://streamlit.io/gallery?category=favorites). As a class, choose your favorite and answer the following **questions:**\n","* Who is this application for?\n","* How does the user input data - are these intuitive ways of interacting with the app?\n","* What does the application do with the data?\n","* Evaluate the ease of use and look of the application.\n","Now that we’ve seen what is possible with Streamlit, let’s try to deploy our **LLMs** to the web!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tn4NbpX0nC1q"},"outputs":[],"source":["# Build an app that demonstrates to the user whether the model is based or not.\n","# use the below code as a basis.\n","# Make changes to the UI using these commands: https://docs.streamlit.io/library/cheatsheet\n","# Use the prompt engineering, model selection and safety methods we have learned."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Pfb5A74MJofA"},"outputs":[],"source":["# @title Run this to install Streamlit!\n","!pip install -q streamlit > /dev/null\n","!pip install pyngrok > /dev/null\n","# !npm install localtunnel\n","from pyngrok import ngrok\n","\n","def launch_website():\n","  print (\"Click this link to try your web app:\")\n","  if (ngrok.get_tunnels() != None):\n","    ngrok.kill()\n","  public_url = ngrok.connect()\n","  print (public_url)\n","  !streamlit run --server.port 80 app.py >/dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDjttyFF0IGM"},"outputs":[],"source":["## App Example\n","\n","%%writefile app.py\n","import streamlit as st\n","import openai as ai\n","\n","API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\n","## Use your own API key: https://platform.openai.com/account/api-keys\n","\n","try:\n","  ai.api_key = \"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\"\n","except:\n","  st.text('Add API Key')\n","\n","def chatgpt_call(prompt, model, temperature):\n","  completion = ai.ChatCompletion.create(\n","    model=model,\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature=temperature\n","  )\n","  return completion['choices'][0]['message']['content']\n","\n","st.header('Example App')\n","topic = st.text_input('Topic you want to learn')\n","model = 'gpt-3.5-turbo' # \"gpt-3.5-turbo\"\n","temperature = 0\n","st.sidebar.markdown(\"This app uses OpenAI's generative AI. Please use it carefully and check any output as it could be biased or wrong. \")\n","\n","prompt = f\"You are an expert teacher. Explain this concept to me as if I am 5 years old: {topic}\"\n","\n","explanation = chatgpt_call(prompt, model, temperature)\n","\n","generate = st.button('Generate Response')\n","\n","if generate:\n","  st.markdown(explanation)\n","  st.balloons()"]},{"cell_type":"markdown","source":["<font color=SlateGrey><h4><b>\n","Use [these](https://drive.google.com/file/d/12zwuOuKh91VSHIHS-6S4ADF4HLC2wKJq/view?usp=sharing) instructions to create a ngrok account and get your authtoken!\n","</b></h2></font>\n","\n","<font color=DarkGray><h5><b>\n","Paste your authtoken below next to `!ngrok authtoken`!\n","</b></h3></font>"],"metadata":{"id":"4G3lOt7OHgVZ"}},{"cell_type":"code","source":["!ngrok authtoken # YOUR AUTH TOKEN HERE"],"metadata":{"id":"UVu9FpavHcGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0FESgR80NR9"},"outputs":[],"source":["# Run Streamlit App\n","launch_website()"]},{"cell_type":"markdown","metadata":{"id":"IVcAejy7uVrv"},"source":["**Final Question**: Is chatGPT safe enough to deploy your app? Why/ Why not?"]},{"cell_type":"markdown","metadata":{"id":"dPC8GCUOHsuH"},"source":["## 9. Knowledge Check\n","\n","1. What are stochastic parrots?\n","\n","2. What is Anthropic's Constitutional AI?\n","\n","**Deeper Questions**\n","1. Can AI-in-the-loop work in practice or is it a form of cyclical logic?"]},{"cell_type":"markdown","metadata":{"id":"vJiFYDid8QTw"},"source":["## 10. Extra Resources\n","\n","1. [Yann Lecun and Andrew Ng](https://www.youtube.com/watch?v=BY9KV8uCtj4&pp=ygUJbGFjdW4gbmdv) think that AI is not an existential threat.\n","2. [Podcast on AI Ethics](https://tib.buzzsprout.com/1597213/10937307-iason-gabriel-artificial-intelligence-and-moral-philosophy) from Iason Gabriel from OpenAI"]}],"metadata":{"colab":{"provenance":[{"file_id":"1DUTS1DvORjoPvGS8X7eUXhCDrW_KBaN0","timestamp":1703801587603}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"efeeed2e6245494ab7844791f5b94c04":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_018e0afc5f5e4bf0800fcab896b07375","max":10,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_631efaa337d8496184586b3b6291f64d","value":5}},"018e0afc5f5e4bf0800fcab896b07375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"631efaa337d8496184586b3b6291f64d":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}}}}},"nbformat":4,"nbformat_minor":0}